{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l66rLvIJTzC3"
   },
   "source": [
    "# Mnist dataset building vanilla auto-encoder using three fully-connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uCD6NUSdT4uq"
   },
   "outputs": [],
   "source": [
    "# import the needed packages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0TR7cgcT8_g",
    "outputId": "3eb5e294-0e89-4bd7-997f-0bbfe6682e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "(X_train_full, _), (X_test, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-wX4klTbUB9t"
   },
   "outputs": [],
   "source": [
    "# split, reshape and normalize the training, validation and test sets\n",
    "X_train = X_train_full[:-10000].reshape(-1, 784)/255.\n",
    "X_valid = X_train_full[-10000:].reshape(-1, 784)/255.\n",
    "X_test = X_test.reshape(-1, 784)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7Djmbi-EUHoq"
   },
   "outputs": [],
   "source": [
    "# define a function that creates a vanilla autoencoder with a specified size for the latent space\n",
    "def built_model(encoding_dim):\n",
    "    # fix the random numbers generator (it would be better to fix also the random.seed and np.random.seed)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # define the input size\n",
    "    input_img = Input(shape = (784,))\n",
    "\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation = 'relu')(input_img)\n",
    "\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(784, activation = 'sigmoid')(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_img, decoded)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fybnc2utULdR",
    "outputId": "102e8829-57ea-4af5-9f6d-5abea91cc56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training with latent space size: 10\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.2177 - val_loss: 0.1704\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1672 - val_loss: 0.1632\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1621 - val_loss: 0.1599\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1591 - val_loss: 0.1574\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1573 - val_loss: 0.1564\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1563 - val_loss: 0.1556\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1557 - val_loss: 0.1550\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1552 - val_loss: 0.1547\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1548 - val_loss: 0.1543\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1545 - val_loss: 0.1541\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1543 - val_loss: 0.1540\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1541 - val_loss: 0.1539\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1540 - val_loss: 0.1538\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1538 - val_loss: 0.1536\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1537 - val_loss: 0.1536\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1536 - val_loss: 0.1534\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1536 - val_loss: 0.1534\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1535 - val_loss: 0.1533\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1534 - val_loss: 0.1533\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1534 - val_loss: 0.1532\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1533 - val_loss: 0.1533\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1533 - val_loss: 0.1531\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1532 - val_loss: 0.1531\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1532 - val_loss: 0.1532\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1531 - val_loss: 0.1530\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1531 - val_loss: 0.1531\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1531 - val_loss: 0.1530\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1531 - val_loss: 0.1530\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.1530 - val_loss: 0.1530\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1530 - val_loss: 0.1530\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "\n",
      "\n",
      "Latent space size:  10, 0.029559\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training with latent space size: 100\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.1308 - val_loss: 0.0876\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0797 - val_loss: 0.0754\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0733 - val_loss: 0.0722\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0710 - val_loss: 0.0708\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0698 - val_loss: 0.0699\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0692 - val_loss: 0.0695\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0688 - val_loss: 0.0690\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0685 - val_loss: 0.0689\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0683 - val_loss: 0.0688\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0682 - val_loss: 0.0687\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0681 - val_loss: 0.0685\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0680 - val_loss: 0.0686\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.0679 - val_loss: 0.0684\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.0679 - val_loss: 0.0684\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0678 - val_loss: 0.0685\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0678 - val_loss: 0.0683\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0677 - val_loss: 0.0683\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0677 - val_loss: 0.0683\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0677 - val_loss: 0.0683\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0677 - val_loss: 0.0682\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0676 - val_loss: 0.0682\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0676 - val_loss: 0.0682\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0676 - val_loss: 0.0682\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0676 - val_loss: 0.0682\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0675 - val_loss: 0.0682\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0675 - val_loss: 0.0682\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0674 - val_loss: 0.0681\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.0674 - val_loss: 0.0680\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0674 - val_loss: 0.0681\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0674 - val_loss: 0.0681\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.0674 - val_loss: 0.0680\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0674 - val_loss: 0.0681\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0674 - val_loss: 0.0682\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "\n",
      "\n",
      "Latent space size: 100, 0.002109\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training with latent space size: 250\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 17s 10ms/step - loss: 0.1065 - val_loss: 0.0750\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0710 - val_loss: 0.0691\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0678 - val_loss: 0.0676\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0666 - val_loss: 0.0666\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0660 - val_loss: 0.0662\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0656 - val_loss: 0.0660\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0653 - val_loss: 0.0657\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.0651 - val_loss: 0.0656\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0649 - val_loss: 0.0655\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 0.0648 - val_loss: 0.0653\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0647 - val_loss: 0.0652\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.0646 - val_loss: 0.0651\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0645 - val_loss: 0.0651\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0645 - val_loss: 0.0650\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0644 - val_loss: 0.0649\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0644 - val_loss: 0.0649\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0643 - val_loss: 0.0649\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0643 - val_loss: 0.0649\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0642 - val_loss: 0.0649\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0642 - val_loss: 0.0648\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0642 - val_loss: 0.0648\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0642 - val_loss: 0.0648\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0642 - val_loss: 0.0647\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0641 - val_loss: 0.0647\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0641 - val_loss: 0.0647\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0641 - val_loss: 0.0648\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0641 - val_loss: 0.0647\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0641 - val_loss: 0.0648\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0641 - val_loss: 0.0647\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "\n",
      "\n",
      "Latent space size: 250, 0.001132\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent_space_sizes = [10, 100, 250]\n",
    "\n",
    "models = {}\n",
    "for latent_size in latent_space_sizes:\n",
    "\n",
    "    print(f\"\\n\\nTraining with latent space size: {latent_size}\\n\\n\")\n",
    "    # create the model\n",
    "    models[latent_size] = built_model(latent_size)\n",
    "\n",
    "    # visualize the model\n",
    "    plot_model(models[latent_size], show_shapes = True)\n",
    "\n",
    "    # compile the model\n",
    "    models[latent_size].compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                      patience=5,\n",
    "                                                      restore_best_weights=True)\n",
    "\n",
    "    history = models[latent_size].fit(X_train, X_train, epochs=100, batch_size = 32,\n",
    "                                      validation_data=(X_valid, X_valid),\n",
    "                                      callbacks=[early_stopping])\n",
    "\n",
    "    reconstructed = models[latent_size].predict(X_test)\n",
    "    mse = mean_squared_error(X_test, reconstructed)\n",
    "\n",
    "    print(f\"\\n\\nLatent space size: {latent_size:3d}, {mse:.6f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T72ySo53UT1q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
